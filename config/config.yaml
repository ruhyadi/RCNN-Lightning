augmentation:
  train:
    augs:
    - class_name: albumentations.Flip
      params:
        p: 0.6
    - class_name: albumentations.RandomBrightnessContrast
      params:
        p: 0.6
    - class_name: albumentations.pytorch.transforms.ToTensorV2
      params:
        p: 1.0
    bbox_params:
      format: pascal_voc
      label_fields:
      - labels
  valid:
    augs:
    - class_name: albumentations.pytorch.transforms.ToTensorV2
      params:
        p: 1.0
    bbox_params:
      format: pascal_voc
      label_fields:
      - labels
callbacks:
  early_stopping:
    class_name: pl.callbacks.EarlyStopping
    params:
      mode: ${training.mode}
      monitor: ${training.metric}
      patience: 10
  model_checkpoint:
    class_name: pl.callbacks.ModelCheckpoint
    params:
      dirpath: saved_models/
      mode: ${training.mode}
      monitor: ${training.metric}
      save_top_k: 3
data:
  batch_size: 12
  folder_path: /content/data
  num_workers: 0
dataset:
  class_name: WheatDataset
general:
  project_name: wheat
  save_dir: logs/
  workspace: erlemar
logging:
  log: true
model:
  backbone:
    class_name: torchvision.models.detection.fasterrcnn_resnet50_fpn
    params:
      pretrained: true
  head:
    class_name: torchvision.models.detection.faster_rcnn.FastRCNNPredictor
    params:
      num_classes: 2
optimizer:
  class_name: torch.optim.AdamW
  params:
    lr: ${training.lr}
    weight_decay: 0.001
private:
  comet_api: fOmVZaafsPuJ6OP3myaJUd4fC
scheduler:
  class_name: torch.optim.lr_scheduler.ReduceLROnPlateau
  monitor: ${training.metric}
  params:
    factor: 0.1
    mode: ${training.mode}
    patience: 5
  step: epoch
trainer:
  accumulate_grad_batches: 1
  gpus: 1
  gradient_clip_val: 0.5
  max_epochs: 10
  num_sanity_val_steps: 0
  profiler: false
training:
  debug: False
  lr: 0.0001
  metric: main_score
  mode: max
  seed: 666